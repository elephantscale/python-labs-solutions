{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 : TextBlob\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 19.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "\n",
    "## downloading using command line\n",
    "#! python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob aims to provide access to common text-processing operations \n",
      "through a familiar interface. You can treat TextBlob objects as if they were Python \n",
      "strings that learned how to do Natural Language Processing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "# setup nltk data\n",
    "# from os.path import expanduser\n",
    "# nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "\n",
    "text = \"\"\"TextBlob aims to provide access to common text-processing operations \n",
    "through a familiar interface. You can treat TextBlob objects as if they were Python \n",
    "strings that learned how to do Natural Language Processing.\n",
    "\"\"\"\n",
    "\n",
    "tb = TextBlob(text)\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TextBlob', 'aims', 'to', 'provide', 'access', 'to', 'common', 'text-processing', 'operations', 'through', 'a', 'familiar', 'interface', 'You', 'can', 'treat', 'TextBlob', 'objects', 'as', 'if', 'they', 'were', 'Python', 'strings', 'that', 'learned', 'how', 'to', 'do', 'Natural', 'Language', 'Processing']\n",
      "\n",
      "[Sentence(\"TextBlob aims to provide access to common text-processing operations \n",
      "through a familiar interface.\"), Sentence(\"You can treat TextBlob objects as if they were Python \n",
      "strings that learned how to do Natural Language Processing.\")]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tb.words)\n",
    "print()\n",
    "\n",
    "print(tb.sentences)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love bigmacs ==> Sentiment(polarity=0.5, subjectivity=0.6)\n",
      "I hate this traffic! ==> Sentiment(polarity=-1.0, subjectivity=0.9)\n",
      "American Idol is awesome! ==> Sentiment(polarity=0.5, subjectivity=0.5)\n",
      "this song is lame ==> Sentiment(polarity=-0.5, subjectivity=0.75)\n",
      "Let's go to beach ==> Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "tweets = [\"I love bigmacs\",  \n",
    "          \"I hate this traffic!\",  \n",
    "          \"American Idol is awesome!\", \n",
    "          \"this song is lame\", \n",
    "          \"Let's go to beach\"]\n",
    "\n",
    "for tweet in tweets:\n",
    "    tb = TextBlob(tweet)\n",
    "    print(\"{} ==> {}\".format(tweet, tb.sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflection and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "dogs\n",
      "men\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "words = [\"cat\", \"dog\", \"man\", \"person\"]\n",
    "for w in words:\n",
    "    print(Word(w).pluralize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "# lematize\n",
    "from textblob import Word\n",
    "print(Word(\"went\").lemmatize('v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet integration / Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an open-source version of the UNIX operating system']\n",
      "\n",
      "- a game played on a court by two opposing teams of 5 players; points are scored by throwing the ball through an elevated horizontal hoop\n",
      "- an inflated ball used in playing basketball\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "print(Word(\"linux\").define())\n",
    "print()\n",
    "\n",
    "for d in Word(\"basketball\").definitions:\n",
    "    print(\"- \" + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'it': 2, 'was': 2, 'a': 3, 'sunny': 2, 'day': 2, 'we': 1, 'went': 1, 'to': 2, 'the': 2, 'dog': 2, 'park': 1, 'lots': 1, 'of': 1, 'dogs': 1, 'were': 1, 'running': 1, 'around': 1, 'my': 1, 'likes': 1, 'run': 1, 'too': 1, 'so': 1, 'he': 1, 'had': 1, 'great': 1, 'time': 1, 'i': 1, 'bought': 1, 'ice': 2, 'cream': 2, 'from': 1, 'truck': 1, 'yummy': 1, 'perfect': 1})\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"It was a sunny day! We went to the dog park.  Lots of dogs were running around.  \n",
    "My dog likes to run too; so he had a great time.  \n",
    "I bought ice cream from the ice cream truck. Yummy!\n",
    "It was a perfect sunny day!\"\"\"\n",
    "\n",
    "tb = TextBlob(text)\n",
    "print(tb.word_counts)\n",
    "print()\n",
    "\n",
    "print(tb.word_counts['sunny'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2 grams\n",
      "[WordList(['It', 'was']),\n",
      " WordList(['was', 'a']),\n",
      " WordList(['a', 'sunny']),\n",
      " WordList(['sunny', 'day']),\n",
      " WordList(['day', 'We']),\n",
      " WordList(['We', 'went']),\n",
      " WordList(['went', 'to']),\n",
      " WordList(['to', 'the']),\n",
      " WordList(['the', 'dog']),\n",
      " WordList(['dog', 'park']),\n",
      " WordList(['park', 'Lots']),\n",
      " WordList(['Lots', 'of']),\n",
      " WordList(['of', 'dogs']),\n",
      " WordList(['dogs', 'were']),\n",
      " WordList(['were', 'running']),\n",
      " WordList(['running', 'around']),\n",
      " WordList(['around', 'My']),\n",
      " WordList(['My', 'dog']),\n",
      " WordList(['dog', 'likes']),\n",
      " WordList(['likes', 'to']),\n",
      " WordList(['to', 'run']),\n",
      " WordList(['run', 'too']),\n",
      " WordList(['too', 'so']),\n",
      " WordList(['so', 'he']),\n",
      " WordList(['he', 'had']),\n",
      " WordList(['had', 'a']),\n",
      " WordList(['a', 'great']),\n",
      " WordList(['great', 'time']),\n",
      " WordList(['time', 'I']),\n",
      " WordList(['I', 'bought']),\n",
      " WordList(['bought', 'ice']),\n",
      " WordList(['ice', 'cream']),\n",
      " WordList(['cream', 'from']),\n",
      " WordList(['from', 'the']),\n",
      " WordList(['the', 'ice']),\n",
      " WordList(['ice', 'cream']),\n",
      " WordList(['cream', 'truck']),\n",
      " WordList(['truck', 'Yummy']),\n",
      " WordList(['Yummy', 'It']),\n",
      " WordList(['It', 'was']),\n",
      " WordList(['was', 'a']),\n",
      " WordList(['a', 'perfect']),\n",
      " WordList(['perfect', 'sunny']),\n",
      " WordList(['sunny', 'day'])]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "text = \"\"\"It was a sunny day! We went to the dog park.  Lots of dogs were running around.  \n",
    "My dog likes to run too; so he had a great time.  \n",
    "I bought ice cream from the ice cream truck. Yummy!\n",
    "It was a perfect sunny day!\"\"\"\n",
    "\n",
    "tb = TextBlob(text)\n",
    "\n",
    "print(\"n=2 grams\")\n",
    "pprint(tb.ngrams(n=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection & Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just had dinner\n",
      "to Spanish : Acabo de cenar \n",
      "to Japanese : 私はちょうど夕食を食べました \n",
      "Language detection : ja \n"
     ]
    }
   ],
   "source": [
    "text_en = \"I just had dinner\"\n",
    "print(text_en)\n",
    "print(\"to Spanish : {} \".format (TextBlob(text_en).translate(to='es')))\n",
    "print(\"to Japanese : {} \".format(TextBlob(text_en).translate(to='ja')))\n",
    "\n",
    "text_jp = u\"私はちょうど夕食\"\n",
    "print(\"Language detection : {} \".format(TextBlob(text_jp).detect_language()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
